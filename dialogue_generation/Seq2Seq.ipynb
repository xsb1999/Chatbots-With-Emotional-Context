{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
    "from keras import callbacks\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPU训练\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "latent_dim = 256 # LSTM 的单元个数\n",
    "vocab = pickle.load(open(\"../models/word_id_map.pkl\",\"rb\"))\n",
    "id2word, word2id = vocab[0], vocab[1]\n",
    "data_all = pickle.load(open(\"../models/train_test_emotion_data.pkl\",\"rb\"))\n",
    "train_x, train_y1, train_y2, train_e, test_x, test_y1, test_y2, test_e = data_all[0], data_all[1], data_all[2], data_all[3], data_all[4], data_all[5], data_all[6], data_all[7]\n",
    "num_samples = len(train_x) # 训练样本的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 50, 3125)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义编码器的输入\n",
    "# encoder_inputs (None, num_encoder_tokens), None表示可以处理任意长度的序列\n",
    "encoder_inputs = Input(shape=(None, train_x.shape[2]), name='encoder_inputs')\n",
    "\n",
    "# 编码器，要求其返回状态（3层LSTM）\n",
    "# 调用编码器，得到编码器的输出（输入其实不需要），以及状态信息 state_h 和 state_c\n",
    "# 丢弃encoder_outputs, 我们只需要编码器的状态\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, name='encoder_lstm1')(encoder_inputs)\n",
    "state_h1 = encoder_lstm1[1]\n",
    "state_c1 = encoder_lstm1[2]\n",
    "encoder_state1 = [state_h1, state_c1]\n",
    "\n",
    "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, name='encoder_lstm2')(encoder_lstm1[0])\n",
    "state_h2 = encoder_lstm2[1]\n",
    "state_c2 = encoder_lstm2[2]\n",
    "encoder_state2 = [state_h2, state_c2]\n",
    "\n",
    "encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, name='encoder_lstm3')(encoder_lstm2[0])\n",
    "state_h3 = encoder_lstm3[1]\n",
    "state_c3 = encoder_lstm3[2]\n",
    "encoder_state3 = [state_h3, state_c3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义解码器的输入\n",
    "# 同样的，None表示可以处理任意长度的序列（3层单向LSTM）\n",
    "# 这里除了真实的answer句子，还拼上了answer的情绪（5维的向量）\n",
    "decoder_inputs = Input(shape=(None, train_x.shape[2]+train_e.shape[1]), name='decoder_inputs')\n",
    "\n",
    "# 接下来建立解码器，解码器将返回整个输出序列（3层单向LSTM）\n",
    "# 并且返回其中间状态，中间状态在训练阶段不会用到，但是在推理阶段将是有用的\n",
    "decoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, name='decoder_lstm1')(decoder_inputs, initial_state=encoder_state1)\n",
    "decoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, name='decoder_lstm2')(decoder_lstm1[0], initial_state=encoder_state2)\n",
    "decoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.2, name='decoder_lstm3')(decoder_lstm2[0], initial_state=encoder_state3)\n",
    "\n",
    "# 将编码器输出的状态作为初始解码器的初始状态\n",
    "decoder_outputs, _, _ = decoder_lstm3\n",
    "\n",
    "# 添加全连接层\n",
    "decoder_dense = Dense(train_x.shape[2], activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义整个模型\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# 编译模型\n",
    "# model.compile(optimizer='adadelta', loss='categorical_crossentropy')\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将emotion数据扩展维度到 (batchSize, 50, 5)\n",
    "def expand_emotion_dim(e_data, MaxLen):\n",
    "    t = np.expand_dims(e_data, axis=1)\n",
    "    t = t.tolist()\n",
    "    for i in range(len(t)):\n",
    "        for j in range(MaxLen-1):\n",
    "            t[i].append(t[i][0])\n",
    "    return np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = expand_emotion_dim(train_e, 50)\n",
    "# 这里将真实的answer句子和answer的情绪拼成一个向量\n",
    "# (只有decoder_input需要，decoder_target(decoder_output进入dense)不用，softmax中不需要拼接情绪向量)\n",
    "# y1是decoder_input(最前面是SOS)\n",
    "concat_y1_emotion = np.concatenate([train_y1,train_e], axis=2)\n",
    "# y2是decoder_output(最后面是EOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**decoder_target_data 与 decoder_input_data 相同，但是有一个时间的偏差。 decoder_target_data[:, t, :] 与decoder_input_data[:, t+1, :]相同**\n",
    "<br>\n",
    "decoder_target_data比与decoder_input_data多了一个EOS<br>\n",
    "decoder_input_data比与decoder_target_data多了一个SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2410, 50, 3130)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_y1_emotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('../models/seq2seq_model_3layers.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1928 samples, validate on 482 samples\n",
      "Epoch 1/120\n",
      "1928/1928 [==============================] - 71s 37ms/step - loss: 1.5434 - val_loss: 1.4087\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40866, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 2/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 1.4253 - val_loss: 1.3957\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40866 to 1.39566, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 3/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 1.4152 - val_loss: 1.3918\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.39566 to 1.39179, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 4/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 1.4083 - val_loss: 1.3960\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.39179\n",
      "Epoch 5/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.4039 - val_loss: 1.4000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.39179\n",
      "Epoch 6/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3993 - val_loss: 1.4177\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.39179\n",
      "Epoch 7/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3956 - val_loss: 1.4015\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.39179\n",
      "Epoch 8/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3939 - val_loss: 1.4119\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.39179\n",
      "Epoch 9/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3881 - val_loss: 1.4084\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.39179\n",
      "Epoch 10/120\n",
      "1928/1928 [==============================] - 65s 33ms/step - loss: 1.3823 - val_loss: 1.3785\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.39179 to 1.37847, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 11/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3580 - val_loss: 1.3818\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.37847\n",
      "Epoch 12/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3343 - val_loss: 1.3382\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.37847 to 1.33820, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 13/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.3176 - val_loss: 1.3159\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.33820 to 1.31587, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 14/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.2882 - val_loss: 1.3117\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.31587 to 1.31167, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 15/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.2705 - val_loss: 1.2778\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.31167 to 1.27775, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 16/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.2540 - val_loss: 1.2768\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.27775 to 1.27682, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 17/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.2381 - val_loss: 1.2820\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.27682\n",
      "Epoch 18/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.2227 - val_loss: 1.2653\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.27682 to 1.26533, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 19/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.2159 - val_loss: 1.2461\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.26533 to 1.24611, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 20/120\n",
      "1928/1928 [==============================] - 63s 32ms/step - loss: 1.2002 - val_loss: 1.2452\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.24611 to 1.24522, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 21/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1906 - val_loss: 1.2455\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.24522\n",
      "Epoch 22/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 1.1778 - val_loss: 1.2419\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.24522 to 1.24195, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 23/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1644 - val_loss: 1.2488\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.24195\n",
      "Epoch 24/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1564 - val_loss: 1.2278\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.24195 to 1.22778, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 25/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1449 - val_loss: 1.2262\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.22778 to 1.22620, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 26/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1346 - val_loss: 1.2287\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.22620\n",
      "Epoch 27/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1262 - val_loss: 1.2255\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.22620 to 1.22547, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 28/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1179 - val_loss: 1.2286\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.22547\n",
      "Epoch 29/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1089 - val_loss: 1.2361\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.22547\n",
      "Epoch 30/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.1000 - val_loss: 1.2309\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.22547\n",
      "Epoch 31/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 1.0880 - val_loss: 1.2235\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.22547 to 1.22355, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 32/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0827 - val_loss: 1.2212\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.22355 to 1.22120, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 33/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0729 - val_loss: 1.2106\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.22120 to 1.21059, saving model to ../models/seq2seq_model_3layers_1.hdf5\n",
      "Epoch 34/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0653 - val_loss: 1.2447\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.21059\n",
      "Epoch 35/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0550 - val_loss: 1.2242\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.21059\n",
      "Epoch 36/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0493 - val_loss: 1.2286\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.21059\n",
      "Epoch 37/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0404 - val_loss: 1.2272\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.21059\n",
      "Epoch 38/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0317 - val_loss: 1.2179\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.21059\n",
      "Epoch 39/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0247 - val_loss: 1.2331\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.21059\n",
      "Epoch 40/120\n",
      "1928/1928 [==============================] - 63s 32ms/step - loss: 1.0150 - val_loss: 1.2275\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.21059\n",
      "Epoch 41/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 1.0090 - val_loss: 1.2331\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.21059\n",
      "Epoch 42/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 1.0050 - val_loss: 1.2253\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.21059\n",
      "Epoch 43/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.9951 - val_loss: 1.2311\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.21059\n",
      "Epoch 44/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.9859 - val_loss: 1.2287\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.21059\n",
      "Epoch 45/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9801 - val_loss: 1.2231\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.21059\n",
      "Epoch 46/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9664 - val_loss: 1.2241\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.21059\n",
      "Epoch 47/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9647 - val_loss: 1.2291\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.21059\n",
      "Epoch 48/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9558 - val_loss: 1.2344\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.21059\n",
      "Epoch 49/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9547 - val_loss: 1.2492\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.21059\n",
      "Epoch 50/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9424 - val_loss: 1.2572\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.21059\n",
      "Epoch 51/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9372 - val_loss: 1.2332\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.21059\n",
      "Epoch 52/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9311 - val_loss: 1.2346\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.21059\n",
      "Epoch 53/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9229 - val_loss: 1.2509\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.21059\n",
      "Epoch 54/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.9165 - val_loss: 1.2571\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.21059\n",
      "Epoch 55/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.9101 - val_loss: 1.2404\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.21059\n",
      "Epoch 56/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.9047 - val_loss: 1.2642\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.21059\n",
      "Epoch 57/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8939 - val_loss: 1.2425\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.21059\n",
      "Epoch 58/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8899 - val_loss: 1.2604\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.21059\n",
      "Epoch 59/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8808 - val_loss: 1.2520\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.21059\n",
      "Epoch 60/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.8714 - val_loss: 1.2543\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.21059\n",
      "Epoch 61/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.8673 - val_loss: 1.2649\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.21059\n",
      "Epoch 62/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.8639 - val_loss: 1.2597\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.21059\n",
      "Epoch 63/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8518 - val_loss: 1.2692\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.21059\n",
      "Epoch 64/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8427 - val_loss: 1.2742\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.21059\n",
      "Epoch 65/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8410 - val_loss: 1.2828\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.21059\n",
      "Epoch 66/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8359 - val_loss: 1.2995\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.21059\n",
      "Epoch 67/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8347 - val_loss: 1.2746\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.21059\n",
      "Epoch 68/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8277 - val_loss: 1.2802\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.21059\n",
      "Epoch 69/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8202 - val_loss: 1.2928\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.21059\n",
      "Epoch 70/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8097 - val_loss: 1.2686\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.21059\n",
      "Epoch 71/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.8051 - val_loss: 1.2928\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.21059\n",
      "Epoch 72/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7931 - val_loss: 1.2899\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.21059\n",
      "Epoch 73/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7877 - val_loss: 1.2903\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.21059\n",
      "Epoch 74/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7805 - val_loss: 1.3057\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.21059\n",
      "Epoch 75/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7829 - val_loss: 1.2947\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.21059\n",
      "Epoch 76/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7764 - val_loss: 1.2919\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.21059\n",
      "Epoch 77/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7655 - val_loss: 1.2974\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.21059\n",
      "Epoch 78/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7623 - val_loss: 1.3102\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.21059\n",
      "Epoch 79/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7561 - val_loss: 1.3211\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.21059\n",
      "Epoch 80/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7462 - val_loss: 1.3235\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.21059\n",
      "Epoch 81/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7413 - val_loss: 1.3058\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.21059\n",
      "Epoch 82/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7388 - val_loss: 1.3065\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.21059\n",
      "Epoch 83/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7325 - val_loss: 1.3283\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.21059\n",
      "Epoch 84/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7296 - val_loss: 1.3309\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.21059\n",
      "Epoch 85/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7247 - val_loss: 1.3173\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.21059\n",
      "Epoch 86/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7133 - val_loss: 1.3365\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.21059\n",
      "Epoch 87/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7113 - val_loss: 1.3277\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.21059\n",
      "Epoch 88/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.7040 - val_loss: 1.3308\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.21059\n",
      "Epoch 89/120\n",
      "1928/1928 [==============================] - 62s 32ms/step - loss: 0.6987 - val_loss: 1.3280\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.21059\n",
      "Epoch 90/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.7014 - val_loss: 1.3458\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.21059\n",
      "Epoch 91/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.6878 - val_loss: 1.3284\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.21059\n",
      "Epoch 92/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.6795 - val_loss: 1.3397\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.21059\n",
      "Epoch 93/120\n",
      "1928/1928 [==============================] - 61s 32ms/step - loss: 0.6750 - val_loss: 1.3454\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.21059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1991ce80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_x, concat_y1_emotion], train_y2,\n",
    "          batch_size=batch_size,\n",
    "          epochs = 120,\n",
    "          callbacks=[early_stopping, checkpoint],\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 3125)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None, 3130)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm1 (LSTM)            [(None, None, 256),  3463168     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm1 (LSTM)            [(None, None, 256),  3468288     decoder_inputs[0][0]             \n",
      "                                                                 encoder_lstm1[0][1]              \n",
      "                                                                 encoder_lstm1[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm2 (LSTM)            [(None, None, 256),  525312      encoder_lstm1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm2 (LSTM)            [(None, None, 256),  525312      decoder_lstm1[0][0]              \n",
      "                                                                 encoder_lstm2[0][1]              \n",
      "                                                                 encoder_lstm2[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm3 (LSTM)            [(None, None, 256),  525312      encoder_lstm2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm3 (LSTM)            [(None, None, 256),  525312      decoder_lstm2[0][0]              \n",
      "                                                                 encoder_lstm3[0][1]              \n",
      "                                                                 encoder_lstm3[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 3125)   803125      decoder_lstm3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 9,835,829\n",
      "Trainable params: 9,835,829\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
