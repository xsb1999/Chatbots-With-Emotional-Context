{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用Keras构建模型，双向双层LSTM\n",
    "## Two-way double layer LSTM (Keras framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Lambda, LSTM, TimeDistributed, Masking, \\\n",
    "    Bidirectional\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os, pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用GPU训练\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pickle.load(open(\"../models/data_word_id_map.pkl\",\"rb\"))\n",
    "train_x, train_y, test_x, test_y, id2word, word2id = data_all[0], data_all[1], data_all[2], data_all[3], data_all[4], data_all[5]\n",
    "wordNum = len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "leng = train_x.shape[1]\n",
    "inputs = Input(shape=(leng,wordNum), dtype='float32')\n",
    "# 双向LSTM\n",
    "lstm = Bidirectional(LSTM(300, activation='relu', return_sequences=True, dropout=0.3))(inputs)\n",
    "lstm = Bidirectional(LSTM(300, activation='relu', return_sequences=False, dropout=0.3), name=\"utter\")(lstm)\n",
    "classes = 5\n",
    "output = Dense(classes, activation='softmax')(lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs, output)\n",
    "model.compile(optimizer='adadelta', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 100\n",
    "checkpoint = ModelCheckpoint('../models/emotion_recognition_model.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5904 samples, validate on 1476 samples\n",
      "Epoch 1/150\n",
      "5904/5904 [==============================] - 104s 18ms/step - loss: 1.5879 - val_loss: 1.5942\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.59423, saving model to ./model_all_3.hdf5\n",
      "Epoch 2/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.5833 - val_loss: 1.6053\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.59423\n",
      "Epoch 3/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.5817 - val_loss: 1.5952\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.59423\n",
      "Epoch 4/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.5787 - val_loss: 1.6012\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.59423\n",
      "Epoch 5/150\n",
      "5904/5904 [==============================] - 102s 17ms/step - loss: 1.5737 - val_loss: 1.5860\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.59423 to 1.58601, saving model to ./model_all_3.hdf5\n",
      "Epoch 6/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.5672 - val_loss: 1.5891\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.58601\n",
      "Epoch 7/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.5521 - val_loss: 1.6552\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.58601\n",
      "Epoch 8/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.5353 - val_loss: 1.5715\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.58601 to 1.57146, saving model to ./model_all_3.hdf5\n",
      "Epoch 9/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.5078 - val_loss: 1.5386\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.57146 to 1.53865, saving model to ./model_all_3.hdf5\n",
      "Epoch 10/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.4784 - val_loss: 1.5502\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.53865\n",
      "Epoch 11/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.4489 - val_loss: 1.6274\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.53865\n",
      "Epoch 12/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.4035 - val_loss: 1.3904\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.53865 to 1.39036, saving model to ./model_all_3.hdf5\n",
      "Epoch 13/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.3776 - val_loss: 1.5606\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.39036\n",
      "Epoch 14/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.3259 - val_loss: 1.8580\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.39036\n",
      "Epoch 15/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.2903 - val_loss: 1.3478\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.39036 to 1.34780, saving model to ./model_all_3.hdf5\n",
      "Epoch 16/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.2389 - val_loss: 1.4599\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.34780\n",
      "Epoch 17/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.2173 - val_loss: 1.4842\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.34780\n",
      "Epoch 18/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.1763 - val_loss: 1.7234\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.34780\n",
      "Epoch 19/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.1559 - val_loss: 1.5455\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.34780\n",
      "Epoch 20/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.1010 - val_loss: 1.4715\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.34780\n",
      "Epoch 21/150\n",
      "5904/5904 [==============================] - 102s 17ms/step - loss: 1.0875 - val_loss: 1.3626\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.34780\n",
      "Epoch 22/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.0559 - val_loss: 1.2718\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.34780 to 1.27177, saving model to ./model_all_3.hdf5\n",
      "Epoch 23/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.0442 - val_loss: 1.2968\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.27177\n",
      "Epoch 24/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1.0210 - val_loss: 1.4485\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.27177\n",
      "Epoch 25/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.9984 - val_loss: 1.2286\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.27177 to 1.22865, saving model to ./model_all_3.hdf5\n",
      "Epoch 26/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.9820 - val_loss: 1.2270\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.22865 to 1.22698, saving model to ./model_all_3.hdf5\n",
      "Epoch 27/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.9779 - val_loss: 1.4318\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.22698\n",
      "Epoch 28/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.9594 - val_loss: 1.4199\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.22698\n",
      "Epoch 29/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.9363 - val_loss: 1.2139\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.22698 to 1.21389, saving model to ./model_all_3.hdf5\n",
      "Epoch 30/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.9311 - val_loss: 1.1965\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.21389 to 1.19649, saving model to ./model_all_3.hdf5\n",
      "Epoch 31/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.8952 - val_loss: 1.1979\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.19649\n",
      "Epoch 32/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.8911 - val_loss: 1.1575\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.19649 to 1.15746, saving model to ./model_all_3.hdf5\n",
      "Epoch 33/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.8713 - val_loss: 1.5787\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.15746\n",
      "Epoch 34/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.8539 - val_loss: 1.1779\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.15746\n",
      "Epoch 35/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.8606 - val_loss: 1.1465\n",
      "\n",
      "Epoch 00035: val_loss improved from 1.15746 to 1.14652, saving model to ./model_all_3.hdf5\n",
      "Epoch 36/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.8538 - val_loss: 1.3526\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.14652\n",
      "Epoch 37/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.8260 - val_loss: 1.4173\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.14652\n",
      "Epoch 38/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.8213 - val_loss: 1.4921\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.14652\n",
      "Epoch 39/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.8038 - val_loss: 2.8037\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.14652\n",
      "Epoch 40/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.8187 - val_loss: 1.1649\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.14652\n",
      "Epoch 41/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.7897 - val_loss: 1.3181\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.14652\n",
      "Epoch 42/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.7760 - val_loss: 1.3824\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.14652\n",
      "Epoch 43/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.7695 - val_loss: 1.3614\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.14652\n",
      "Epoch 44/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.7724 - val_loss: 1.4542\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.14652\n",
      "Epoch 45/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.7376 - val_loss: 1.2930\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.14652\n",
      "Epoch 46/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.7397 - val_loss: 1.5144\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.14652\n",
      "Epoch 47/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.7307 - val_loss: 1.6249\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.14652\n",
      "Epoch 48/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.7303 - val_loss: 1.5124\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.14652\n",
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.7230 - val_loss: 1.2286\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.14652\n",
      "Epoch 50/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.7106 - val_loss: 1.3512\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.14652\n",
      "Epoch 51/150\n",
      "5904/5904 [==============================] - 99s 17ms/step - loss: 0.7088 - val_loss: 1.8954\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.14652\n",
      "Epoch 52/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6909 - val_loss: 1.3047\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.14652\n",
      "Epoch 53/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6909 - val_loss: 1.2046\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.14652\n",
      "Epoch 54/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6841 - val_loss: 1.5241\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.14652\n",
      "Epoch 55/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6920 - val_loss: 1.3198\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.14652\n",
      "Epoch 56/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6752 - val_loss: 1.2259\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.14652\n",
      "Epoch 57/150\n",
      "5904/5904 [==============================] - 99s 17ms/step - loss: 0.6631 - val_loss: 1.2047\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.14652\n",
      "Epoch 58/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6667 - val_loss: 1.3664\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.14652\n",
      "Epoch 59/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6658 - val_loss: 1.2208\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.14652\n",
      "Epoch 60/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6462 - val_loss: 1.6452\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.14652\n",
      "Epoch 61/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6578 - val_loss: 1.2192\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.14652\n",
      "Epoch 62/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6535 - val_loss: 1.3472\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.14652\n",
      "Epoch 63/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6233 - val_loss: 1.4019\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.14652\n",
      "Epoch 64/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6348 - val_loss: 1.2993\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.14652\n",
      "Epoch 65/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6332 - val_loss: 1.7853\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.14652\n",
      "Epoch 66/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6222 - val_loss: 1.3794\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.14652\n",
      "Epoch 67/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5966 - val_loss: 1.3338\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.14652\n",
      "Epoch 68/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6050 - val_loss: 1.5167\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.14652\n",
      "Epoch 69/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5985 - val_loss: 1.3355\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.14652\n",
      "Epoch 70/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.6006 - val_loss: 1.2483\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.14652\n",
      "Epoch 71/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5904 - val_loss: 1.3154\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.14652\n",
      "Epoch 72/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5934 - val_loss: 1.3342\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.14652\n",
      "Epoch 73/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5924 - val_loss: 1.3073\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.14652\n",
      "Epoch 74/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5837 - val_loss: 1.2742\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.14652\n",
      "Epoch 75/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5873 - val_loss: 1.4441\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.14652\n",
      "Epoch 76/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5796 - val_loss: 1.4710\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.14652\n",
      "Epoch 77/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5642 - val_loss: 1.3592\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.14652\n",
      "Epoch 78/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5688 - val_loss: 1.3438\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.14652\n",
      "Epoch 79/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5639 - val_loss: 1.4621\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.14652\n",
      "Epoch 80/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5727 - val_loss: 1.2889\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.14652\n",
      "Epoch 81/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5520 - val_loss: 1.7018\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.14652\n",
      "Epoch 82/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5537 - val_loss: 1.3995\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.14652\n",
      "Epoch 83/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5399 - val_loss: 1.4345\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.14652\n",
      "Epoch 84/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5635 - val_loss: 1.2589\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.14652\n",
      "Epoch 85/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5409 - val_loss: 1.5637\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.14652\n",
      "Epoch 86/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5385 - val_loss: 1.4067\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.14652\n",
      "Epoch 87/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5445 - val_loss: 1.5020\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.14652\n",
      "Epoch 88/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.5353 - val_loss: 1.4200\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.14652\n",
      "Epoch 89/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5403 - val_loss: 1.4050\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.14652\n",
      "Epoch 90/150\n",
      "5904/5904 [==============================] - 105s 18ms/step - loss: 0.5306 - val_loss: 1.4995\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.14652\n",
      "Epoch 91/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5089 - val_loss: 1.4919\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.14652\n",
      "Epoch 92/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5298 - val_loss: 1.5556\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.14652\n",
      "Epoch 93/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5322 - val_loss: 1.5040\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.14652\n",
      "Epoch 94/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5155 - val_loss: 1.3164\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.14652\n",
      "Epoch 95/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5117 - val_loss: 1.3272\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.14652\n",
      "Epoch 96/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5109 - val_loss: 1.4304\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.14652\n",
      "Epoch 97/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5104 - val_loss: 1.4221\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.14652\n",
      "Epoch 98/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 193.9569 - val_loss: 1.4963\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.14652\n",
      "Epoch 99/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5207 - val_loss: 1.3355\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.14652\n",
      "Epoch 100/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4965 - val_loss: 1.3437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00100: val_loss did not improve from 1.14652\n",
      "Epoch 101/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4776 - val_loss: 1.4077\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.14652\n",
      "Epoch 102/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4647 - val_loss: 1.3862\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.14652\n",
      "Epoch 103/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4739 - val_loss: 1.4352\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.14652\n",
      "Epoch 104/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4792 - val_loss: 1.4626\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.14652\n",
      "Epoch 105/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4730 - val_loss: 1.3830\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.14652\n",
      "Epoch 106/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4728 - val_loss: 1.4005\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.14652\n",
      "Epoch 107/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4553 - val_loss: 1.4667\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.14652\n",
      "Epoch 108/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4893 - val_loss: 1.4982\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.14652\n",
      "Epoch 109/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4751 - val_loss: 1.6026\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.14652\n",
      "Epoch 110/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4590 - val_loss: 1.5994\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.14652\n",
      "Epoch 111/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4687 - val_loss: 1.4888\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.14652\n",
      "Epoch 112/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4726 - val_loss: 1.6219\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.14652\n",
      "Epoch 113/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4767 - val_loss: 1.4049\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.14652\n",
      "Epoch 114/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4682 - val_loss: 1.3814\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.14652\n",
      "Epoch 115/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4773 - val_loss: 1.4393\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.14652\n",
      "Epoch 116/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.4724 - val_loss: 1.4464\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.14652\n",
      "Epoch 117/150\n",
      "5904/5904 [==============================] - 103s 17ms/step - loss: 1601635122.7255 - val_loss: 64517302.3997\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.14652\n",
      "Epoch 118/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 14586920.4176 - val_loss: 3.5417\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.14652\n",
      "Epoch 119/150\n",
      "5904/5904 [==============================] - 103s 18ms/step - loss: 183880.8838 - val_loss: 489.0697\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.14652\n",
      "Epoch 120/150\n",
      "5904/5904 [==============================] - 102s 17ms/step - loss: 639477.0320 - val_loss: 1220.9797\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.14652\n",
      "Epoch 121/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 304282.5411 - val_loss: 21.4778\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.14652\n",
      "Epoch 122/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 5964.2515 - val_loss: 1.4872\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.14652\n",
      "Epoch 123/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 1041.7914 - val_loss: 1.2648\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.14652\n",
      "Epoch 124/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.9444 - val_loss: 1.2254\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.14652\n",
      "Epoch 125/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.6423 - val_loss: 1.2106\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.14652\n",
      "Epoch 126/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 1.9193 - val_loss: 1.2124\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.14652\n",
      "Epoch 127/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 6.6261 - val_loss: 1.2435\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.14652\n",
      "Epoch 128/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 70.5204 - val_loss: 1.2835\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.14652\n",
      "Epoch 129/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.5583 - val_loss: 1.2575\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.14652\n",
      "Epoch 130/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.5645 - val_loss: 1.3330\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.14652\n",
      "Epoch 131/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5483 - val_loss: 1.2777\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.14652\n",
      "Epoch 132/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.5460 - val_loss: 1.2889\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.14652\n",
      "Epoch 133/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.5199 - val_loss: 1.3396\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.14652\n",
      "Epoch 134/150\n",
      "5904/5904 [==============================] - 101s 17ms/step - loss: 0.5351 - val_loss: 1.3527\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.14652\n",
      "Epoch 135/150\n",
      "5904/5904 [==============================] - 100s 17ms/step - loss: 0.5298 - val_loss: 1.4336\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.14652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a2bbb70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "          callbacks=[early_stopping, checkpoint],\n",
    "                  shuffle=True,\n",
    "         validation_data=(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2.2_gpu]",
   "language": "python",
   "name": "conda-env-tf2.2_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
